{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a custom container and Estimator to run Catboost on SageMaker\n",
    "\n",
    "In this notebook, we use the SageMaker Training Toolkit (https://github.com/aws/sagemaker-training-toolkit) to create a SageMaker-compatible docker image to run python scripts using the Catboost algorithm library. We also show how to create a custom SageMaker training `Estimator` from the SageMaker `Framework` class (https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Framework)\n",
    "\n",
    "CatBoost is a high-performance open source library for gradient boosting on decision trees. You can learn more about it at the following links:\n",
    "* https://tech.yandex.com/catboost/\n",
    "* https://catboost.ai/\n",
    "* https://github.com/catboost/catboost\n",
    "\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "We use the Boston Housing dataset, present in Scikit-Learn: https://scikit-learn.org/stable/datasets/index.html#boston-dataset\n",
    "\n",
    "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978. Used in Belsley, Kuh & Welsch, 'Regression diagnostics ...', Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.\n",
    "\n",
    "The Boston house-price data has been used in many machine learning papers that address regression problems.\n",
    "\n",
    "References\n",
    "\n",
    " * Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
    " * Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
    "\n",
    "**This sample is provided for demonstration purposes, make sure to conduct appropriate testing if derivating this code for your own use-cases!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Container creation and upload to Amazon ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a SageMaker-compatible Catboost container\n",
    "We derive our dockerfile from the SageMaker Scikit-Learn dockerfile https://github.com/aws/sagemaker-scikit-learn-container/blob/master/docker/0.20.0/base/Dockerfile.cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM ubuntu:16.04\n",
    "\n",
    "RUN apt-get update && \\\n",
    "    apt-get -y install build-essential libatlas-dev git wget curl nginx jq libatlas3-base\n",
    "\n",
    "RUN curl -LO http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh && \\\n",
    "    bash Miniconda3-latest-Linux-x86_64.sh -bfp /miniconda3 && \\\n",
    "    rm Miniconda3-latest-Linux-x86_64.sh\n",
    "\n",
    "ENV PATH=/miniconda3/bin:${PATH}\n",
    "        \n",
    "RUN apt-get update && apt-get install -y python-pip && pip install sagemaker-training catboost scikit-learn setuptools wheel spacy && python -m spacy download en_core_web_sm\n",
    "\n",
    "ENV PYTHONDONTWRITEBYTECODE=1 PYTHONUNBUFFERED=1 PYTHONIOENCODING=UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending the container to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "ecr_namespace = 'sagemaker-training-containers/'\n",
    "prefix = 'catboost-image'\n",
    "\n",
    "ecr_repository_name = ecr_namespace + prefix\n",
    "account_id = role.split(':')[4]\n",
    "region = boto3.Session().region_name\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "print('Account: {}'.format(account_id))\n",
    "print('Region: {}'.format(region))\n",
    "print('Role: {}'.format(role))\n",
    "print('S3 Bucket: {}'.format(bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build_and_push.sh\n",
    "\n",
    "ACCOUNT_ID=$1\n",
    "REGION=$2\n",
    "REPO_NAME=$3\n",
    "\n",
    "\n",
    "sudo docker build -f Dockerfile -t $REPO_NAME .\n",
    "\n",
    "docker tag $REPO_NAME $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPO_NAME:latest\n",
    "\n",
    "$(aws ecr get-login --no-include-email --registry-ids $ACCOUNT_ID)\n",
    "\n",
    "aws ecr describe-repositories --repository-names $REPO_NAME || aws ecr create-repository --repository-name $REPO_NAME\n",
    "\n",
    "docker push $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPO_NAME:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash build_and_push.sh $account_id $region $ecr_repository_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com/{2}:latest'.format(account_id, region, ecr_repository_name)\n",
    "print('ECR container ARN: {}'.format(container_image_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The docker image is now pushed to ECR and is ready for consumption! In the next section, we go in the shoes of an ML practitioner that develops a Catboost model and runs it remotely on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: local ML development and remote training job with Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We install catboost locally for local development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost \n",
    "!pip install scikit-optimize\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "We use pandas to process a small local dataset into a training and testing piece.\n",
    "\n",
    "We could also design code that loads all the data and runs cross-validation within the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing a local training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://black-belt-ml-challenge.s3.us-east-2.amazonaws.com/wines.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import spacy\n",
    "#nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wines.csv')\n",
    "df.loc[:,'log1p_price'] = np.log1p(df.price)\n",
    "df.loc[:,'len_description']=df.description.str.len()\n",
    "df.loc[:,'len_title']=df.title.str.len()\n",
    "df.loc[:,'len_winery']=df.winery.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[:,'doc'] = df.apply(lambda x : nlp(x.description),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good_vectors = [ 1,  2,  3,  5,  6, 11, 14, 15, 16, 20, 21, 22, 24, 25, 26, 27, 28,\n",
    "#       29, 30, 31, 32, 33, 36, 37, 38, 39, 41, 42, 43, 45, 46, 47, 48, 49,\n",
    "#       51, 52, 53, 54, 58, 61, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73,\n",
    "#       74, 75, 76, 77, 79, 81, 82, 84, 85, 87, 88, 89, 90, 91, 93, 94, 95]\n",
    "#df_tensor_sum = df.apply(lambda x : x.doc.tensor[:,good_vectors].sum(axis=0),axis=1,result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tensor_sum.columns = ['tensor_value_'+str(i) for i in df_tensor_sum.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.concat([df,df_tensor_sum],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[:,'max_tensor'] = df.doc.apply(lambda x : x.tensor.max())\n",
    "#df.loc[:,'sum_tensor'] = df.doc.apply(lambda x : x.tensor.sum())\n",
    "#df.loc[:,'count_ents']= df.doc.apply(lambda x : len([ent for ent in x.ents]))\n",
    "#df.loc[:,'count_ADJ']= df.doc.apply(lambda x : len([token.pos_ for token in x if token.pos_ =='ADJ']))\n",
    "#df.loc[:,'count_is_not_stop']= df.doc.apply(lambda x : len([token.pos_ for token in x if token.is_stop==False ]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[:,'contains_ripe']= df.doc.apply(lambda x : len([token for token in x if token.pos_ =='ADJ' and  token.is_stop==False and token.lemma_=='ripe']))\n",
    "#df.loc[:,'contains_red']= df.doc.apply(lambda x : len([token for token in x if token.pos_ =='ADJ' and  token.is_stop==False and token.lemma_=='red']))\n",
    "#df.loc[:,'contains_rich']= df.doc.apply(lambda x : len([token for token in x if token.pos_ =='ADJ' and  token.is_stop==False and token.lemma_=='rich']))\n",
    "#df.loc[:,'contains_fresh']= df.doc.apply(lambda x : len([token for token in x if token.pos_ =='ADJ' and  token.is_stop==False and token.lemma_=='fresh']))\n",
    "#df.loc[:,'contains_soft']= df.doc.apply(lambda x : len([token for token in x if token.pos_ =='ADJ' and  token.is_stop==False and token.lemma_=='soft']))\n",
    "#df.loc[:,'contains_sweet']= df.doc.apply(lambda x : len([token for token in x if token.pos_ =='ADJ' and  token.is_stop==False and token.lemma_=='sweet']))\n",
    "#df.loc[:,'contains_green']= df.doc.apply(lambda x : len([token for token in x if token.pos_ =='ADJ' and  token.is_stop==False and token.lemma_=='green']))\n",
    "#df.loc[:,'contains_simple']= df.doc.apply(lambda x : len([token for token in x if token.pos_ =='ADJ' and  token.is_stop==False and token.lemma_=='simple']))\n",
    "#df.loc[:,'contains_light']= df.doc.apply(lambda x : len([token for token in x if token.pos_ =='ADJ' and  token.is_stop==False and token.lemma_=='light']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = ['description', 'designation']\n",
    "\n",
    "cat_features = ['country', 'province', 'region_1', 'region_2', \n",
    "                'taster_name', 'taster_twitter_handle', 'variety',\n",
    "                'winery']\n",
    "\n",
    "df.loc[:,cat_features] = df.loc[:,cat_features].fillna('Missing')\n",
    "\n",
    "df.loc[:,text_features] = df.loc[:,text_features].fillna('Missing')\n",
    "#df = df.drop(columns=['doc', 'title'])\n",
    "df = df.drop(columns=['title'])\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_train='wines_train.csv'\n",
    "local_test='wines_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(local_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(local_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send data to S3. SageMaker will take training data from S3\n",
    "train_location = sess.upload_data(\n",
    "    path=local_train, \n",
    "    bucket=bucket,\n",
    "    key_prefix='catboost')\n",
    "\n",
    "test_location = sess.upload_data(\n",
    "    path=local_test, \n",
    "    bucket=bucket,\n",
    "    key_prefix='catboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile catboost_training_wines.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool, cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    print('extracting arguments')\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--train-file', type=str, default='wines_train.csv')\n",
    "    parser.add_argument('--test-file', type=str, default='wines_test.csv')\n",
    "    parser.add_argument('--model-name', type=str, default='catboost_model.dump')\n",
    "    parser.add_argument('--features', type=str)  # in this script we ask user to explicitly name features\n",
    "    parser.add_argument('--cat_features', type=str)  # in this script we ask user to explicitly name cat_features\n",
    "    parser.add_argument('--target', type=str) # in this script we ask user to explicitly name the target\n",
    "    parser.add_argument('--learning_rate', type=float) # in this script we ask user to explicitly name the target\n",
    "    parser.add_argument('--depth', type=int) # in this script we ask user to explicitly name the target\n",
    "    parser.add_argument('--l2_leaf_reg', type=int) # in this script we ask user to explicitly name the target\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    logging.info('reading data')\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "\n",
    "    logging.info('building training and testing datasets')\n",
    "    X_train = train_df[args.features.split()]\n",
    "    X_test = test_df[args.features.split()]\n",
    "    y_train = train_df[args.target]\n",
    "    y_test = test_df[args.target]\n",
    "        \n",
    "    # define and train model\n",
    "    #model = CatBoostRegressor(learning_rate=args.learning_rate,depth=args.depth,l2_leaf_reg=args.l2_leaf_reg,cat_features=args.cat_features.split())\n",
    "    #\n",
    "    #model.fit(X_train, y_train, eval_set=(X_test, y_test), logging_level='Silent') \n",
    "    #\n",
    "    ## print abs error\n",
    "    #logging.info('validating model')\n",
    "    #abs_err = np.abs(model.predict(X_test) - y_test)\n",
    "    #preds = model.predict(X_test).round(0)\n",
    "    #models_evals = {'explained_variance_score' : [metrics.explained_variance_score(y_test, preds)],\n",
    "    #            'max_error' : [metrics.max_error(y_test, preds)],\n",
    "    #            'mean_absolute_error' : [metrics.mean_absolute_error(y_test, preds)],\n",
    "    #            'root_mean_squared_error' : [metrics.mean_squared_error(y_test, preds)**(1/2)],\n",
    "    #            'mean_squared_error' : [metrics.mean_squared_error(y_test, preds)],\n",
    "    #            'mean_squared_log_error' : [metrics.mean_squared_log_error(y_test, preds)],\n",
    "    #            'median_absolute_error' : [metrics.median_absolute_error(y_test, preds)],\n",
    "    #            #metrics.mean_absolute_percentage_error(y_test, preds),\n",
    "    #            'r2_score' : [metrics.r2_score(y_test, preds)]}\n",
    "    \n",
    "        # print couple perf metrics\n",
    "    #for q in models_evals.keys():\n",
    "    #    logging.info(str(q)+' : '+ str(models_evals[q]))\n",
    "    \n",
    "    cv_dataset = Pool(data=X_train,\n",
    "                  label=y_train,\n",
    "                  cat_features=args.cat_features.split())\n",
    "\n",
    "    params = {\"iterations\": 1000,\n",
    "              \"learning_rate\":args.learning_rate,\n",
    "              \"depth\": args.depth,\n",
    "              \"loss_function\": \"RMSE\",\n",
    "              \"l2_leaf_reg\": args.l2_leaf_reg,\n",
    "              \"verbose\": False}\n",
    "\n",
    "    scores = cv(cv_dataset,\n",
    "                params,\n",
    "                fold_count=3, \n",
    "            )\n",
    "    \n",
    "    logging.info('rmse'+': '+ str(scores['test-RMSE-mean'].iloc[-1]))\n",
    "    # print couple perf metrics\n",
    "    #for q in [10, 50, 90]:\n",
    "    #    logging.info('AE-at-' + str(q) + 'th-percentile: '\n",
    "    #          + str(np.percentile(a=abs_err, q=q)))\n",
    "    \n",
    "    # persist model\n",
    "    #path = os.path.join(args.model_dir, args.model_name)\n",
    "    #logging.info('saving to {}'.format(path))\n",
    "    #model.save_model(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our script locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_str=' '.join([i for i in df_train.columns if i not in ('points')])\n",
    "features_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_str = ' '.join([i for i in df_train.columns if i in cat_features+text_features])\n",
    "cat_features_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# local test\n",
    "\n",
    "! python catboost_training_wines.py \\\n",
    "    --train ./ \\\n",
    "    --test ./ \\\n",
    "    --model-dir ./ \\\n",
    "    --features 'country description designation price province region_1 region_2 taster_name taster_twitter_handle variety winery log1p_price len_description len_title len_winery' \\\n",
    "    --cat_features 'country description designation province region_1 region_2 taster_name taster_twitter_handle variety winery' \\\n",
    "    --target 'points' \\\n",
    "    --learning_rate 0.1 \\\n",
    "    --depth 4 \\\n",
    "    --l2_leaf_reg 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote training in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Launch a SageMaker training job from code uploaded to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that option, we first need to send code to S3. This could also be done automatically by a build system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first compress the code and send to S3\n",
    "program = 'catboost_training_wines.py'\n",
    "source = 'source.tar.gz'\n",
    "project = 'catboost'\n",
    "\n",
    "tar = tarfile.open(source, 'w:gz')\n",
    "tar.add(program)\n",
    "tar.close()\n",
    "\n",
    "submit_dir = sess.upload_data(\n",
    "    path=source, \n",
    "    bucket=bucket,\n",
    "    key_prefix=project+ '/' + source)\n",
    "\n",
    "print(submit_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then launch a training job with the `Estimator` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 's3://' + bucket + '/' + project + '/' + 'training_jobs'\n",
    "\n",
    "estimator = Estimator(image_uri=container_image_uri,\n",
    "                      role=role,\n",
    "                      max_run=20*60,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.m5.xlarge',\n",
    "                      output_path=output_path,\n",
    "                      use_spot_instances=True,\n",
    "                      max_wait=20*60,\n",
    "                      hyperparameters={'sagemaker_program': program,\n",
    "                                       'sagemaker_submit_directory': submit_dir,\n",
    "                                       'features': features_str,\n",
    "                                       'cat_features': cat_features_str,\n",
    "                                       'target': 'points'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator.fit({'train':train_location, 'test': test_location}, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.01, 0.1, scaling_type=\"Logarithmic\"),\n",
    "    \"depth\": IntegerParameter(4, 10),\n",
    "    \"l2_leaf_reg\": IntegerParameter(1, 9),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = \"rmse\"\n",
    "metric_definitions = [{\"Name\": \"rmse\", \"Regex\": \"rmse: ([0-9\\\\.]+)\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    strategy='Bayesian',\n",
    "    objective_type=\"Minimize\",\n",
    "    max_jobs=50,\n",
    "    max_parallel_jobs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tuner.fit({'train':train_location, 'test': test_location},logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.HyperparameterTuningJobAnalytics(tuner.latest_tuning_job.job_name).dataframe().sort_values(['FinalObjectiveValue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
